{"extracted_information": "The provided web content, a review titled 'AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and Challenges,' extensively discusses challenges and limitations related to AI agent coherence, context drift, task switching, and maintaining focus and consistency, particularly in Section V titled 'Challenges and Limitations in AI Agents and Agentic AI.'", "specifications": {}, "pricing": {}, "features": [], "statistics": {}, "temporal_info": {}, "geographical_data": {}, "references": [{"point": "AI Agents: Limited Long-Horizon Planning and Recovery", "details": "A persistent limitation of current AI Agents lies in their inability to perform robust long-horizon planning, especially in complex, multi-stage tasks. This constraint stems from their foundational reliance on stateless prompt-response paradigms, where each decision is made without an intrinsic memory of prior reasoning steps unless externally managed. Although augmentations such as the ReAct framework or Tree-of-Thoughts introduce pseudo-recursive reasoning, they remain fundamentally heuristic and lack true internal models of time, causality, or state evolution. Consequently, agents often falter in tasks requiring extended temporal consistency or contingency planning. They may exhibit repetitive behaviors such as endlessly querying tools or fail to adapt when sub-tasks fail or return ambiguous results. The absence of systematic recovery mechanisms or error detection leads to brittle workflows and error propagation. This shortfall severely limits agent deployment in mission-critical environments where reliability, fault tolerance, and sequential coherence are essential. (Section V-1, Point 4)", "solutions_mentioned": ["ReAct framework", "Tree-of-Thoughts"]}, {"point": "AI Agents: Incomplete Agentic Properties (Autonomy, Proactivity, Reactivity, Social Ability)", "details": "Autonomy is typically partial, with agents relying heavily on external scaffolding (human-defined prompts, planning heuristics, feedback loops). Self-initiated task generation, self-monitoring, or autonomous error correction are rare or absent. Proactivity is underdeveloped; most AI Agents require explicit user instruction and lack the capacity to formulate or reprioritize goals dynamically. Reactivity is constrained by architectural bottlenecks like response latency due to repeated LLM inference calls and narrow contextual memory windows, inhibiting real-time adaptability. Social ability is largely absent, with brittle, template-based dialogue lacking long-term memory integration or nuanced conversational context; agent-to-agent interaction is often hardcoded or limited. (Section V-1, Point 3)", "solutions_mentioned": []}, {"point": "AI Agents: Inherited Limitations from LLMs (Hallucination, Brittleness, Shallow Reasoning, Static Knowledge, Bias)", "details": "AI Agents powered by LLMs inherit issues like hallucinations (plausible but factually incorrect outputs), which can lead to misjudgments in high-stakes domains. They exhibit prompt sensitivity, where minor phrasing variations cause divergent behaviors, hampering reproducibility and requiring meticulous manual prompt engineering. While reasoning heuristics (Chain-of-Thought, ReAct) simulate deliberation, they remain shallow in semantic comprehension, leading to failures in multi-step inference, misalignment of task objectives, or logically inconsistent conclusions. LLMs have static knowledge cutoffs and cannot dynamically integrate new information unless augmented. They also reproduce biases from training datasets. (Section V-1, Point 2)", "solutions_mentioned": ["Retrieval-Augmented Generation (RAG)", "Tool-Augmented Reasoning (Function Calling)", "Memory Architectures (Episodic, Semantic, Vector)", "Programmatic Prompt Engineering Pipelines"]}, {"point": "Agentic AI: Communication and Coordination Bottlenecks (Goal Alignment, Shared Context, Protocol Limitations, Resource Contention)", "details": "Achieving efficient communication and coordination across multiple autonomous agents is a fundamental challenge. Agents often lack a unified semantic understanding of overarching objectives, hampering sub-task decomposition, dependency management, and progress monitoring, especially in dynamic environments. Most systems rely on natural language exchanges over loosely defined interfaces, prone to ambiguity, inconsistent formatting, and contextual drift. Resource contention emerges when agents simultaneously access shared computational, memory, or API resources, leading to race conditions, execution delays, or system failures. (Section V-2, Point 2)", "solutions_mentioned": ["Multi-Agent Orchestration with Role Specialization", "Programmatic Prompt Engineering Pipelines"]}, {"point": "Agentic AI: Emergent Behavior and Predictability", "details": "Managing emergent behavior (complex system-level phenomena from agent interactions) introduces significant unpredictability and safety risks. Unintended outcomes can arise, potentially diverging from task objectives, generating misleading outputs, or enacting harmful actions. System instability (infinite planning loops, action deadlocks, contradictory behaviors) increases with the number and complexity of agents. The stochasticity and opacity of LLM-based agents make internal decision logic difficult to interpret or verify. (Section V-2, Point 3)", "solutions_mentioned": ["Reflexive and Self-Critique Mechanisms", "Causal Modeling and Simulation-Based Planning", "Monitoring, Auditing, and Explainability Pipelines", "Governance-Aware Architectures (Accountability and Role Isolation)"]}, {"point": "Agentic AI: Scalability and Debugging Complexity", "details": "As Agentic AI systems scale, maintaining reliability and interpretability becomes complex. Black-box chains of reasoning in LLM-based agents, involving opaque internal logic, tool invocations, and inter-agent communication, make tracing the root cause of failures non-trivial. The systemâ€™s non-compositionality means adding components can increase cognitive load, noise, and coordination overhead, leading to redundant computation, contradictory decisions, or degraded task performance without robust frameworks. (Section V-2, Point 4)", "solutions_mentioned": ["Monitoring, Auditing, and Explainability Pipelines"]}]}