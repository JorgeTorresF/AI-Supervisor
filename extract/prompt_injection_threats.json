{"extracted_information": "Comprehensive information regarding prompt injection attacks, context manipulation vulnerabilities, and how various triggers (keywords, data poisoning, etc.) can induce unwanted behavior changes in AI agents. The threats are categorized into Input Manipulation, Model Compromise (including backdoors and poisoning), System & Privacy Attacks, and Protocol Vulnerabilities.", "specifications": {"llm_agent_threat_model_domains": ["Input Manipulation", "Model Compromise", "System and Privacy Attacks", "Protocol Vulnerabilities"], "model_context_protocol_mcp": {"description": "Standardized bridge between a language model and external resources for real-time context augmentation and tool invocation.", "attack_vectors": ["Prompt-based injections (direct, P2SQL, indirect/compositional, cross-agent)", "Backdoor mechanisms (prompt-level, model-parameter, composite)", "Fuzzing attacks (jailbreak fuzzing, automated jailbreak-prompt generation)", "Data poisoning (retrieval, medical misinformation, gradient-based, federated model)", "Privacy-extraction (membership inference, side-channel attacks)", "Replay and DoS (infinite tool-call loops, request floods)", "Credential theft (token theft/replay, proxy abuse, insecure configuration)", "Ransomware"]}, "agent_to_agent_protocol_a2a": {"description": "Orchestrates secure, task-oriented interactions among multiple autonomous agents using JSON-RPC-over-HTTP(S) and Server-Sent Events.", "attack_vectors": ["Cross-agent prompt injection", "Agent discovery spoofing", "Rogue agent registration", "Adaptive indirect injections", "Context manipulation", "Contagious recursive blocking", "Memory subversion"]}}, "pricing": {}, "features": [{"attack_type": "Prompt Injection Attacks", "description": "Techniques that subvert LLM behavior by tampering with inputs (text, code, multimodal data) before they reach the core model.", "sub_types": [{"name": "Direct Prompt Injection", "authors": "Perez et al. [33]", "details": "Malicious user interactions, simple handcrafted inputs can induce goal hijacking and prompt leaking in GPT-3. Exploits stochastic behavior."}, {"name": "Prompt-to-SQL (P2SQL) Injection", "authors": "Pedro et al. [34]", "details": "Exploits unsanitized user prompts in LLM-integrated web applications (e.g., Langchain) to inject malicious SQL queries. Widely susceptible (7 SOTA LLMs)."}, {"name": "Indirect & Compositional Prompt Injection", "authors": "Bagdasaryan et al. [35]", "details": "Adversarial perturbations subtly integrated into images or audio recordings to steer multimodal LLM outputs (LLaVA, PandaGPT)."}, {"name": "Adaptive Indirect Prompt Injection", "authors": "Zhan et al. [16]", "details": "Bypasses existing defenses (8 evaluated) with adaptive attack strategies on LLM agents using external tools."}, {"name": "Toxic Agent Flow Attack", "authors": "Invariant Labs 111.", "details": "Exploits GitHub MCP server; malicious GitHub issues cause agents to leak private repository data. Traditional security measures (alignment, prompt defenses) fail."}, {"name": "Cross-Agent Prompt Injection", "details": "Crafted prompts propagate through multi-agent workflows, causing downstream agents to perform unintended actions. (A2A protocol vulnerability)"}]}, {"attack_type": "Context Manipulation & Memory Vulnerabilities", "description": "Exploits related to how LLMs and AI agents handle and store conversational context and memory, leading to unintended behaviors or data leaks.", "sub_types": [{"name": "Context Manipulation Attack", "authors": "Patlan et al. [43]", "details": "Exploits unprotected surfaces within an agentâ€™s input channels, memory modules, and external data feeds in Web3 ecosystems. Can lead to unintended asset transfers or protocol violations. Prompt-based defenses ineffective; fine-tuning-based defenses are more robust."}, {"name": "Long-Context Jailbreak Attack", "authors": "Anil et al. [38]", "details": "Prompts LLMs with hundreds of demonstrations of undesirable behavior, exploiting expanded context windows. Attack success follows a power law, increasing with demonstration count."}, {"name": "Memory-Poisoning Attack (MINJA)", "authors": "Dong et al. [52]", "details": "Targets the memory bank of LLM-based agents, injecting malicious records via queries to cause harmful reasoning and outputs. Highly effective with minimal execution requirements."}]}, {"attack_type": "Keyword Triggers & Unwanted Behavior (Backdoor & Poisoning Attacks)", "description": "Attacks where specific inputs (keywords, poisoned data) or conditions activate pre-implanted malicious behaviors, leading to unintended or harmful outcomes.", "sub_types": [{"name": "Agent Backdoor Attack (BadAgent)", "authors": "Yang et al. [44]", "details": "Manipulates intermediate reasoning steps in LLM-based agents. Triggers can be embedded in user queries or intermediate observations. Difficult to detect as final outputs may appear correct."}, {"name": "Prompt-Level Backdoors (BadPrompt)", "authors": "Cai et al. [45]", "details": "Generates subtle, task-adaptive backdoor triggers for continuous prompt-based learning models in few-shot scenarios using Trigger Candidate Generation (TCG) and Adaptive Trigger Optimization (ATO)."}, {"name": "Prompt-Based Backdoors (PoisonPrompt)", "authors": "Yao et al. [46]", "details": "Compromises prompt integrity across hard and soft prompts, activated by subtle triggers in the input query while maintaining normal behavior otherwise. Affects Prompt-as-a-Service (PraaS)."}, {"name": "Model-Parameter Backdoors (BadAgent)", "authors": "Wang et al. [47]", "details": "Embedded during the fine-tuning process of LLM agents, allowing manipulation to perform harmful operations using specific input or environmental cues. Remains effective even after fine-tuning with trustworthy data."}, {"name": "Composite Backdoor Attack (CBA)", "authors": "Huang et al. [21]", "details": "Scatters multiple trigger keys across different prompt components; activated only when all keys are present. Highly stealthy and effective."}, {"name": "Encrypted Multi-Backdoor Implantation (DemonAgent)", "authors": "Zhu et al. [22]", "details": "Implants backdoors using dynamic encryption and fragmented sub-backdoor fragments, mapping backdoor signals into benign content to evade detection."}, {"name": "Medical Misinformation Poisoning", "authors": "Alber et al. [48]", "details": "Small percentage (0.001%) of token poisoning in training data (The Pile dataset) significantly increases likelihood of harmful output. Corrupted models often perform similarly on standard benchmarks."}, {"name": "Retrieval Poisoning (PoisonedRAG)", "authors": "Zou et al. [23], Zhang et al. [49]", "details": "Knowledge corruption attack targeting RAG systems by injecting malicious texts into external knowledge bases, causing LLMs to generate target answers for specific questions. Documents can be visually indistinguishable from benign ones."}, {"name": "Gradient-Based Backdoor Poisoning", "authors": "Wallace et al. [50]", "details": "Manipulates NLP model predictions through subtle, concealed changes to training data. Specific trigger phrase in input can cause misclassification (e.g., 'James Bond' making sentiment model predict 'Positive')."}, {"name": "Federated Local Model Poisoning", "authors": "Fang et al. [51]", "details": "Compromises client devices in federated learning to manipulate local model parameters, leading to significantly higher error rates for the global model."}]}], "statistics": {"attack_success_rates": [{"attack": "Adaptive Indirect Prompt Injection", "authors": "Zhan et al. [16]", "asr": ">= 50%"}, {"attack": "Compositional Instruction Attack", "authors": "Jiang et al. [17]", "asr": "> 95%"}, {"attack": "Jailbreak Fuzzing", "authors": "Yu et al. [18]", "asr": "> 90%"}, {"attack": "Graph of Attacks with Pruning", "authors": "Schwartz et al. [19]", "asr": "92%", "queries_reduced": "54%"}, {"attack": "Active Environment Injection Attack", "authors": "Chen et al. [20]", "asr": "93% (on AndroidWorld)"}, {"attack": "Composite Backdoor Attack", "authors": "Huang et al. [21]", "asr": "100%", "poisoning_rate": "3% (on Emotion dataset)"}, {"attack": "Encrypted Multi-Backdoor Implantation", "authors": "Zhu et al. [22]", "asr": "approx. 100%", "detection_rate": "0%"}, {"attack": "Retrieval Poisoning", "authors": "Zou et al. [23]", "asr": "90% (by injecting 5 malicious texts into millions of texts)"}, {"attack": "Retrieval Poisoning (real-world)", "authors": "Zhang et al. [49]", "asr": "88.33% (controlled), 66.67% (real-world)"}, {"attack": "Datastore Leakage Attack", "authors": "Qi et al. [54]", "asr": "100% (on 25 customized GPT models)", "extraction_rate": "41% (from 77k-word text), 3% (from 1.5M-word corpus) using 100 queries"}, {"attack": "Medical Misinformation Poisoning", "authors": "Alber et al. [48]", "poisoning_rate": "0.001% (token poisoning)", "impact": "Significantly increases harmful output likelihood"}]}, "temporal_info": {"publication_date": "29 Jun 2025"}, "geographical_data": {}, "references": ["Perez et al. [33]", "Pedro et al. [34]", "Bagdasaryan et al. [35]", "Zhan et al. [16]", "Invariant Labs 111.", "Patlan et al. [43]", "Anil et al. [38]", "Dong et al. [52]", "Yang et al. [44]", "Cai et al. [45]", "Yao et al. [46]", "Wang et al. [47]", "Huang et al. [21]", "Zhu et al. [22]", "Alber et al. [48]", "Zou et al. [23]", "Zhang et al. [49]", "Wallace et al. [50]", "Fang et al. [51]", "Qi et al. [54]"]}