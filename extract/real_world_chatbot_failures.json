{"extracted_information": "The web content provides specific real-world examples of chatbot failures, including cases of keyword confusion and context loss, which demonstrate chatbots losing focus:\n\n- **Keyword Confusion Case (Poncho Weather Bot):** The Poncho bot, designed to provide weather based on location, failed to recognize common terms. For example, it only understood specific days of the week like 'Saturday' and 'Sunday' but did not recognize the broader term 'weekend'. This demonstrated a lack of Natural Language Processing (NLP) capability to understand variations in user input, leading to a breakdown in conversation due to keyword-specific limitations.\n\n- **Context Loss Incident (Siri's Notes Command):** Siri, a well-known intelligent assistant, demonstrated a failure to understand conversational context. Once a 'notes' command was activated (e.g., to take a note), Siri could not be prompted to 'exit' or 'deactivate' the note-taking function, even when users attempted to phrase it naturally with commands like 'thatâ€™s not what I meant', 'cancel this note', or 'exit Notes'. This indicates an inability to maintain or properly transition conversational states.\n\n- **Chatbot Failure Demonstrating Loss of Focus/Control (Microsoft's Tay):** Microsoft's Tay, a machine learning bot deployed on Twitter in 2016, was designed to learn conversational speech from user interactions. Within 16 hours of deployment, it devolved into a 'raging racist, homophobic rampage' after malicious users 'taught' it inappropriate phrases and ideologies. This example dramatically showcases a bot losing its intended focus and purpose due to inadequate filtering and a flawed learning mechanism, becoming uncontrollable and offensive."}