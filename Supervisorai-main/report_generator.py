#!/usr/bin/env python3
"""
Periodic Report Generator for Supervisor Agent
Generates task completion reports, performance analytics, and automated distribution.
"""

import json
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, asdict
from jinja2 import Template
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import io
import base64

@dataclass
class TaskReport:
    """Task completion report structure"""
    task_id: str
    task_name: str
    status: str  # 'completed', 'failed', 'in_progress', 'cancelled'
    start_time: str
    end_time: Optional[str]
    duration_seconds: Optional[float]
    agent_id: str
    confidence_score: Optional[float]
    error_count: int
    retry_count: int
    metadata: Dict[str, Any]

@dataclass
class PerformanceMetrics:
    """Performance analytics structure"""
    period_start: str
    period_end: str
    total_tasks: int
    completed_tasks: int
    failed_tasks: int
    success_rate: float
    average_duration: float
    average_confidence: float
    error_rate: float
    top_errors: List[Dict[str, Any]]
    agent_performance: Dict[str, Any]
    trend_data: Dict[str, List[float]]

class ReportTemplate:
    """Report template management"""
    
    TASK_REPORT_TEMPLATE = """
# Supervisor Agent Task Report

**Report Period:** {{ period_start }} to {{ period_end }}
**Generated:** {{ generation_time }}

## Executive Summary

- **Total Tasks:** {{ metrics.total_tasks }}
- **Success Rate:** {{ "%.1f%%"|format(metrics.success_rate * 100) }}
- **Average Duration:** {{ "%.2f"|format(metrics.average_duration) }}s
- **Average Confidence:** {{ "%.1f%%"|format(metrics.average_confidence * 100) }}
- **Error Rate:** {{ "%.2f%%"|format(metrics.error_rate * 100) }}

## Task Breakdown

### Completed Tasks ({{ metrics.completed_tasks }})
{% for task in completed_tasks[:10] %}
- **{{ task.task_name }}** ({{ task.task_id }})
  - Duration: {{ "%.2f"|format(task.duration_seconds or 0) }}s
  - Confidence: {{ "%.1f%%"|format((task.confidence_score or 0) * 100) }}
  - Agent: {{ task.agent_id }}
{% endfor %}

### Failed Tasks ({{ metrics.failed_tasks }})
{% for task in failed_tasks[:5] %}
- **{{ task.task_name }}** ({{ task.task_id }})
  - Agent: {{ task.agent_id }}
  - Errors: {{ task.error_count }}
  - Retries: {{ task.retry_count }}
{% endfor %}

## Performance Analytics

### Top Errors
{% for error in metrics.top_errors[:5] %}
1. **{{ error.type }}** ({{ error.count }} occurrences)
   - {{ error.description }}
{% endfor %}

### Agent Performance
{% for agent_id, perf in metrics.agent_performance.items() %}
**{{ agent_id }}:**
- Tasks: {{ perf.task_count }}
- Success Rate: {{ "%.1f%%"|format(perf.success_rate * 100) }}
- Avg Duration: {{ "%.2f"|format(perf.avg_duration) }}s
- Avg Confidence: {{ "%.1f%%"|format(perf.avg_confidence * 100) }}

{% endfor %}

## Trends and Insights

### Performance Trends
{% if metrics.trend_data %}
- **Success Rate Trend:** {{ trend_analysis.success_rate }}
- **Duration Trend:** {{ trend_analysis.duration }}
- **Confidence Trend:** {{ trend_analysis.confidence }}
{% endif %}

### Recommendations
{% for recommendation in recommendations %}
- {{ recommendation }}
{% endfor %}

## Detailed Task List

{% for task in all_tasks %}
### {{ task.task_name }} ({{ task.task_id }})
- **Status:** {{ task.status }}
- **Agent:** {{ task.agent_id }}
- **Duration:** {{ "%.2f"|format(task.duration_seconds or 0) }}s
- **Confidence:** {{ "%.1f%%"|format((task.confidence_score or 0) * 100) }}
- **Errors:** {{ task.error_count }}
- **Start Time:** {{ task.start_time }}
- **End Time:** {{ task.end_time or 'N/A' }}

{% endfor %}

---
*Report generated by Supervisor Agent Reporting System*
    """

class PerformanceAnalyzer:
    """Analyzes performance data and generates insights"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    def analyze_tasks(self, tasks: List[TaskReport], 
                     period_start: str, period_end: str) -> PerformanceMetrics:
        """Analyze task performance and generate metrics"""
        if not tasks:
            return self._empty_metrics(period_start, period_end)
        
        total_tasks = len(tasks)
        completed_tasks = len([t for t in tasks if t.status == 'completed'])
        failed_tasks = len([t for t in tasks if t.status == 'failed'])
        
        success_rate = completed_tasks / total_tasks if total_tasks > 0 else 0
        
        # Calculate averages
        durations = [t.duration_seconds for t in tasks if t.duration_seconds]
        confidences = [t.confidence_score for t in tasks if t.confidence_score]
        
        avg_duration = sum(durations) / len(durations) if durations else 0
        avg_confidence = sum(confidences) / len(confidences) if confidences else 0
        
        # Error analysis
        error_count = sum(t.error_count for t in tasks)
        error_rate = error_count / total_tasks if total_tasks > 0 else 0
        
        top_errors = self._analyze_errors(tasks)
        agent_performance = self._analyze_agent_performance(tasks)
        trend_data = self._generate_trend_data(tasks)
        
        return PerformanceMetrics(
            period_start=period_start,
            period_end=period_end,
            total_tasks=total_tasks,
            completed_tasks=completed_tasks,
            failed_tasks=failed_tasks,
            success_rate=success_rate,
            average_duration=avg_duration,
            average_confidence=avg_confidence,
            error_rate=error_rate,
            top_errors=top_errors,
            agent_performance=agent_performance,
            trend_data=trend_data
        )
    
    def _empty_metrics(self, period_start: str, period_end: str) -> PerformanceMetrics:
        """Return empty metrics for periods with no tasks"""
        return PerformanceMetrics(
            period_start=period_start,
            period_end=period_end,
            total_tasks=0,
            completed_tasks=0,
            failed_tasks=0,
            success_rate=0,
            average_duration=0,
            average_confidence=0,
            error_rate=0,
            top_errors=[],
            agent_performance={},
            trend_data={}
        )
    
    def _analyze_errors(self, tasks: List[TaskReport]) -> List[Dict[str, Any]]:
        """Analyze error patterns"""
        error_counts = {}
        for task in tasks:
            if task.error_count > 0 and 'errors' in task.metadata:
                for error in task.metadata['errors']:
                    error_type = error.get('type', 'Unknown')
                    if error_type not in error_counts:
                        error_counts[error_type] = {
                            'count': 0,
                            'description': error.get('message', 'No description')
                        }
                    error_counts[error_type]['count'] += 1
        
        return [
            {
                'type': error_type,
                'count': data['count'],
                'description': data['description']
            }
            for error_type, data in sorted(
                error_counts.items(), 
                key=lambda x: x[1]['count'], 
                reverse=True
            )
        ]
    
    def _analyze_agent_performance(self, tasks: List[TaskReport]) -> Dict[str, Any]:
        """Analyze performance by agent"""
        agent_stats = {}
        
        for task in tasks:
            agent_id = task.agent_id
            if agent_id not in agent_stats:
                agent_stats[agent_id] = {
                    'tasks': [],
                    'task_count': 0,
                    'completed': 0,
                    'failed': 0,
                    'total_duration': 0,
                    'total_confidence': 0,
                    'confidence_count': 0
                }
            
            stats = agent_stats[agent_id]
            stats['tasks'].append(task)
            stats['task_count'] += 1
            
            if task.status == 'completed':
                stats['completed'] += 1
            elif task.status == 'failed':
                stats['failed'] += 1
            
            if task.duration_seconds:
                stats['total_duration'] += task.duration_seconds
            
            if task.confidence_score is not None:
                stats['total_confidence'] += task.confidence_score
                stats['confidence_count'] += 1
        
        # Calculate performance metrics
        performance = {}
        for agent_id, stats in agent_stats.items():
            performance[agent_id] = {
                'task_count': stats['task_count'],
                'success_rate': stats['completed'] / stats['task_count'] if stats['task_count'] > 0 else 0,
                'avg_duration': stats['total_duration'] / stats['task_count'] if stats['task_count'] > 0 else 0,
                'avg_confidence': stats['total_confidence'] / stats['confidence_count'] if stats['confidence_count'] > 0 else 0
            }
        
        return performance
    
    def _generate_trend_data(self, tasks: List[TaskReport]) -> Dict[str, List[float]]:
        """Generate trend data for visualization"""
        if not tasks:
            return {}
        
        # Sort tasks by time
        sorted_tasks = sorted(tasks, key=lambda t: t.start_time)
        
        # Group by hour for trend analysis
        hourly_data = {}
        for task in sorted_tasks:
            hour_key = task.start_time[:13]  # YYYY-MM-DD HH
            if hour_key not in hourly_data:
                hourly_data[hour_key] = {
                    'tasks': [],
                    'completed': 0,
                    'total': 0
                }
            
            hourly_data[hour_key]['tasks'].append(task)
            hourly_data[hour_key]['total'] += 1
            if task.status == 'completed':
                hourly_data[hour_key]['completed'] += 1
        
        # Generate trend arrays
        hours = sorted(hourly_data.keys())
        success_rates = []
        avg_durations = []
        avg_confidences = []
        
        for hour in hours:
            data = hourly_data[hour]
            success_rate = data['completed'] / data['total'] if data['total'] > 0 else 0
            success_rates.append(success_rate)
            
            durations = [t.duration_seconds for t in data['tasks'] if t.duration_seconds]
            avg_duration = sum(durations) / len(durations) if durations else 0
            avg_durations.append(avg_duration)
            
            confidences = [t.confidence_score for t in data['tasks'] if t.confidence_score]
            avg_confidence = sum(confidences) / len(confidences) if confidences else 0
            avg_confidences.append(avg_confidence)
        
        return {
            'hours': hours,
            'success_rates': success_rates,
            'avg_durations': avg_durations,
            'avg_confidences': avg_confidences
        }

class TrendAnalyzer:
    """Analyzes trends and provides insights"""
    
    def analyze_trends(self, trend_data: Dict[str, List[float]]) -> Dict[str, str]:
        """Analyze trends and provide text descriptions"""
        if not trend_data or not trend_data.get('success_rates'):
            return {}
        
        success_rates = trend_data['success_rates']
        avg_durations = trend_data.get('avg_durations', [])
        avg_confidences = trend_data.get('avg_confidences', [])
        
        analysis = {}
        
        # Analyze success rate trend
        if len(success_rates) >= 2:
            trend = self._calculate_trend(success_rates)
            analysis['success_rate'] = self._describe_trend(trend, 'Success rate')
        
        # Analyze duration trend
        if len(avg_durations) >= 2:
            trend = self._calculate_trend(avg_durations)
            analysis['duration'] = self._describe_trend(trend, 'Average duration', inverse=True)
        
        # Analyze confidence trend
        if len(avg_confidences) >= 2:
            trend = self._calculate_trend(avg_confidences)
            analysis['confidence'] = self._describe_trend(trend, 'Confidence')
        
        return analysis
    
    def _calculate_trend(self, values: List[float]) -> float:
        """Calculate simple linear trend"""
        if len(values) < 2:
            return 0
        
        x = list(range(len(values)))
        y = values
        
        n = len(values)
        sum_x = sum(x)
        sum_y = sum(y)
        sum_xy = sum(x[i] * y[i] for i in range(n))
        sum_x2 = sum(x_val ** 2 for x_val in x)
        
        slope = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x ** 2)
        return slope
    
    def _describe_trend(self, slope: float, metric_name: str, 
                       inverse: bool = False) -> str:
        """Convert slope to descriptive text"""
        threshold = 0.01
        
        if abs(slope) < threshold:
            return f"{metric_name} is stable"
        
        if slope > 0:
            direction = "decreasing" if inverse else "increasing"
        else:
            direction = "increasing" if inverse else "decreasing"
        
        if abs(slope) > 0.1:
            intensity = "rapidly"
        elif abs(slope) > 0.05:
            intensity = "steadily"
        else:
            intensity = "slowly"
        
        return f"{metric_name} is {intensity} {direction}"

class RecommendationEngine:
    """Generates recommendations based on performance data"""
    
    def generate_recommendations(self, metrics: PerformanceMetrics, 
                                trend_analysis: Dict[str, str]) -> List[str]:
        """Generate actionable recommendations"""
        recommendations = []
        
        # Success rate recommendations
        if metrics.success_rate < 0.8:
            recommendations.append(
                f"Success rate is {metrics.success_rate:.1%}. Consider investigating top errors and improving error handling."
            )
        
        # Duration recommendations
        if metrics.average_duration > 30:  # 30 seconds threshold
            recommendations.append(
                f"Average task duration is {metrics.average_duration:.1f}s. Consider optimizing slow operations."
            )
        
        # Confidence recommendations
        if metrics.average_confidence < 0.7:
            recommendations.append(
                f"Average confidence is {metrics.average_confidence:.1%}. Review decision-making processes."
            )
        
        # Error rate recommendations
        if metrics.error_rate > 0.1:
            recommendations.append(
                f"Error rate is {metrics.error_rate:.1%}. Focus on the top {len(metrics.top_errors[:3])} error types."
            )
        
        # Agent performance recommendations
        if metrics.agent_performance:
            worst_agent = min(
                metrics.agent_performance.items(),
                key=lambda x: x[1]['success_rate']
            )
            if worst_agent[1]['success_rate'] < 0.6:
                recommendations.append(
                    f"Agent '{worst_agent[0]}' has low success rate ({worst_agent[1]['success_rate']:.1%}). Consider retraining or debugging."
                )
        
        # Trend-based recommendations
        for metric, trend in trend_analysis.items():
            if "decreasing" in trend and metric in ["success_rate", "confidence"]:
                recommendations.append(
                    f"Performance trend shows {trend.lower()}. Immediate attention required."
                )
            elif "increasing" in trend and metric == "duration":
                recommendations.append(
                    f"Task duration trend shows {trend.lower()}. Investigate performance bottlenecks."
                )
        
        return recommendations

class PeriodicReportGenerator:
    """Main periodic report generator"""
    
    def __init__(self, output_dir: str = "reports"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.analyzer = PerformanceAnalyzer()
        self.trend_analyzer = TrendAnalyzer()
        self.recommendation_engine = RecommendationEngine()
        
        # Setup logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
    
    def generate_report(self, tasks: List[TaskReport], 
                       period_start: str, period_end: str,
                       report_type: str = "markdown") -> str:
        """Generate comprehensive performance report"""
        self.logger.info(f"Generating {report_type} report for {len(tasks)} tasks")
        
        # Analyze performance
        metrics = self.analyzer.analyze_tasks(tasks, period_start, period_end)
        
        # Analyze trends
        trend_analysis = self.trend_analyzer.analyze_trends(metrics.trend_data)
        
        # Generate recommendations
        recommendations = self.recommendation_engine.generate_recommendations(
            metrics, trend_analysis
        )
        
        # Prepare template data
        completed_tasks = [t for t in tasks if t.status == 'completed']
        failed_tasks = [t for t in tasks if t.status == 'failed']
        
        template_data = {
            'period_start': period_start,
            'period_end': period_end,
            'generation_time': datetime.now().isoformat(),
            'metrics': metrics,
            'completed_tasks': completed_tasks,
            'failed_tasks': failed_tasks,
            'all_tasks': tasks,
            'trend_analysis': trend_analysis,
            'recommendations': recommendations
        }
        
        # Generate report
        if report_type == "markdown":
            return self._generate_markdown_report(template_data)
        elif report_type == "json":
            return self._generate_json_report(template_data)
        else:
            raise ValueError(f"Unsupported report type: {report_type}")
    
    def _generate_markdown_report(self, data: Dict[str, Any]) -> str:
        """Generate markdown report"""
        template = Template(ReportTemplate.TASK_REPORT_TEMPLATE)
        return template.render(**data)
    
    def _generate_json_report(self, data: Dict[str, Any]) -> str:
        """Generate JSON report"""
        # Convert dataclasses to dict for JSON serialization
        json_data = {
            'report_metadata': {
                'period_start': data['period_start'],
                'period_end': data['period_end'],
                'generation_time': data['generation_time']
            },
            'metrics': asdict(data['metrics']),
            'tasks': [asdict(task) for task in data['all_tasks']],
            'trend_analysis': data['trend_analysis'],
            'recommendations': data['recommendations']
        }
        
        return json.dumps(json_data, indent=2, default=str)
    
    def save_report(self, report_content: str, filename: str) -> str:
        """Save report to file"""
        filepath = self.output_dir / filename
        with open(filepath, 'w') as f:
            f.write(report_content)
        
        self.logger.info(f"Report saved to {filepath}")
        return str(filepath)
    
    def generate_and_save_report(self, tasks: List[TaskReport], 
                                period_start: str, period_end: str,
                                report_type: str = "markdown") -> str:
        """Generate and save report in one step"""
        report = self.generate_report(tasks, period_start, period_end, report_type)
        
        # Generate filename
        start_date = period_start[:10]  # YYYY-MM-DD
        end_date = period_end[:10]
        extension = "md" if report_type == "markdown" else "json"
        filename = f"supervisor_report_{start_date}_to_{end_date}.{extension}"
        
        return self.save_report(report, filename)

# Demo and testing functions
def create_demo_tasks() -> List[TaskReport]:
    """Create demo task reports for testing"""
    tasks = []
    base_time = datetime.now() - timedelta(hours=24)
    
    for i in range(50):
        start_time = base_time + timedelta(minutes=i*10)
        
        # Simulate various task outcomes
        if i % 7 == 0:  # Some failures
            status = 'failed'
            end_time = start_time + timedelta(seconds=30)
            confidence = None
            error_count = 2
        elif i % 15 == 0:  # Some in progress
            status = 'in_progress'
            end_time = None
            confidence = 0.6
            error_count = 0
        else:  # Mostly completed
            status = 'completed'
            end_time = start_time + timedelta(seconds=20 + i % 30)
            confidence = 0.8 + (i % 20) / 100
            error_count = 0
        
        duration = (end_time - start_time).total_seconds() if end_time else None
        
        task = TaskReport(
            task_id=f"task_{i:03d}",
            task_name=f"Process Data Batch {i}",
            status=status,
            start_time=start_time.isoformat(),
            end_time=end_time.isoformat() if end_time else None,
            duration_seconds=duration,
            agent_id=f"agent_{i % 3}",
            confidence_score=confidence,
            error_count=error_count,
            retry_count=error_count,
            metadata={
                'batch_size': 100 + (i % 50),
                'errors': [
                    {'type': 'ConnectionError', 'message': 'Database timeout'}
                ] if error_count > 0 else []
            }
        )
        
        tasks.append(task)
    
    return tasks

if __name__ == '__main__':
    # Demo usage
    generator = PeriodicReportGenerator("demo_reports")
    
    # Create demo data
    demo_tasks = create_demo_tasks()
    
    # Generate reports
    period_start = (datetime.now() - timedelta(days=1)).isoformat()
    period_end = datetime.now().isoformat()
    
    # Markdown report
    md_report_path = generator.generate_and_save_report(
        demo_tasks, period_start, period_end, "markdown"
    )
    print(f"Markdown report saved to: {md_report_path}")
    
    # JSON report
    json_report_path = generator.generate_and_save_report(
        demo_tasks, period_start, period_end, "json"
    )
    print(f"JSON report saved to: {json_report_path}")
